{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "The purpose of this notebook, along with 01_data_setup_example.ipynb and 03_demo_explore_results.ipynb, is to provide a tutorial of how you may want to use the Pop_Exp pacakge functions.\n",
    "\n",
    "Please see 01_data_setup_example.ipynb before you work through this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline\n",
    "\n",
    "To recap, our goal was to demo all the functions available in Pop_Exp. In this notebook we'll do five separate things, which align with the five options available in three functions in the package. \n",
    "\n",
    "First we'll do:\n",
    "\n",
    "0. Setup, and then:\n",
    "\n",
    "1. Find the total number of people residing within 10km of *ANY* US wildfire \n",
    "disaster in 2016, 2017, and 2018. \n",
    "2. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018.\n",
    "3. Find the total number of people residing within 10km of *ANY* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "5. Find the population of all 2020 ZCTAs. \n",
    "\n",
    "In the last notebook, we prepared the wildfire disaster exposure data and ZCTA data to pass to the Pop_Exp functions so we could complete these computations. Here, we'll complete each of them in this order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Setup\n",
    "\n",
    "We need to import some libraries and also install and import Pop_Exp. If you haven't installed Pop_Exp in the environment you're working in now, go ahead and activate that environment, and pip install pop_exp in the terminal. We can then import the functions within Pop_Exp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing necessary libraries.\n",
    "import pathlib\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "# Here's the pop_exp import \n",
    "from Pop_Exp import find_exposure as ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set some paths to make it easy to access the data we cleaned for \n",
    "this tutorial. \n",
    "\n",
    "To find the number of people affected by one or more wildfire disaster by year 2016-2018, and by ZCTA, we need to get the paths to each of our wildfire files that we made in the data setup notebook.\n",
    "\n",
    "The regular expression below selects all the files in the interim data directory that have 'fire' in the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "base_path = pathlib.Path.cwd().parent\n",
    "data_dir = base_path / \"demo_data\"\n",
    "# wf paths regex\n",
    "wildfire_paths = glob.glob(str(data_dir / \"02_interim_data\" / \"*fire*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the path to the population raster we're using, and the ZCTA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GHSL pop raster\n",
    "ghsl_path = data_dir / \"01_raw_data\" / \"GHS_POP_E2020_GLOBE_R2023A_54009_100_V1_0.tif\"\n",
    "\n",
    "# ZCTA path \n",
    "zcta_path = glob.glob(str(data_dir / \"02_interim_data\" / \"*zcta*\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now set up to run the five cases we're interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. \n",
    "\n",
    "Our first goal was to find the total number of people residing within 10 km of *EACH* US wildfire disaster in 2016, 2017, and 2018.\n",
    "\n",
    "To do this, we can run find_number_of_people_affected with the parameter \n",
    "by_unique_hazrad = False. \n",
    "\n",
    "Because we're looping over three years, we'll initialize an empty list first, \n",
    "and then store the results in this list. We're also adding a year variable to \n",
    "the result as we go. In total, this takes around 24 seconds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running find_num_people_affected\n",
      "Reading data and finding best UTM projection for hazard geometries (1/4)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m start_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2016\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     num_affected \u001b[38;5;241m=\u001b[39m ex\u001b[38;5;241m.\u001b[39mfind_num_people_affected(\n\u001b[1;32m      6\u001b[0m         path_to_hazards\u001b[38;5;241m=\u001b[39mwildfire_paths[i],\n\u001b[1;32m      7\u001b[0m         raster_path\u001b[38;5;241m=\u001b[39mghsl_path,\n\u001b[1;32m      8\u001b[0m         by_unique_hazard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# setting by unique hazard to false \u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     num_affected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m start_year \u001b[38;5;241m+\u001b[39m i\n\u001b[1;32m     11\u001b[0m     num_affected_list\u001b[38;5;241m.\u001b[39mappend(num_affected)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/pop_exp/find_exposure.py:303\u001b[0m, in \u001b[0;36mfind_num_people_affected\u001b[0;34m(path_to_hazards, raster_path, by_unique_hazard)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning find_num_people_affected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# prep data\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# get ID, hazard geom, best UTM, buffer dist, and buffered geom in WGS84\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m ch_df \u001b[38;5;241m=\u001b[39m prep_data(path_to_hazards\u001b[38;5;241m=\u001b[39mpath_to_hazards)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# find overlapping buffered hazards\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# select and rename columns in filtered_ch - ID, and set buffered hazard to be geom\u001b[39;00m\n\u001b[1;32m    307\u001b[0m ch_df \u001b[38;5;241m=\u001b[39m ch_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID_climate_hazard\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffered_hazard\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/pop_exp/find_exposure.py:163\u001b[0m, in \u001b[0;36mprep_data\u001b[0;34m(path_to_hazards, path_to_additional_geos)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprep_data\u001b[39m(\n\u001b[1;32m    158\u001b[0m     path_to_hazards: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    159\u001b[0m     path_to_additional_geos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m ):\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# prep both geographies\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     ch_shp \u001b[38;5;241m=\u001b[39m prep_geographies(path_to_hazards, geo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhazard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# if additional_geos isn't None, do this step too\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_to_additional_geos:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/pop_exp/find_exposure.py:112\u001b[0m, in \u001b[0;36mprep_geographies\u001b[0;34m(shp_path, geo_type)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# reproject to WGS84\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shp_df\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     shp_df \u001b[38;5;241m=\u001b[39m shp_df\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# if hazard, add best projection\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geo_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhazard\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/geopandas/geodataframe.py:1701\u001b[0m, in \u001b[0;36mGeoDataFrame.to_crs\u001b[0;34m(self, crs, epsg, inplace)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1700\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 1701\u001b[0m geom \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mto_crs(crs\u001b[38;5;241m=\u001b[39mcrs, epsg\u001b[38;5;241m=\u001b[39mepsg)\n\u001b[1;32m   1702\u001b[0m df\u001b[38;5;241m.\u001b[39mgeometry \u001b[38;5;241m=\u001b[39m geom\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/geopandas/geoseries.py:1207\u001b[0m, in \u001b[0;36mGeoSeries.to_crs\u001b[0;34m(self, crs, epsg)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_crs\u001b[39m(\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28mself\u001b[39m, crs: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, epsg: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GeoSeries:\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a ``GeoSeries`` with all geometries transformed to a new\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    coordinate reference system.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \n\u001b[1;32m   1205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeoSeries(\n\u001b[0;32m-> 1207\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mto_crs(crs\u001b[38;5;241m=\u001b[39mcrs, epsg\u001b[38;5;241m=\u001b[39mepsg), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1208\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/geopandas/_compat.py:90\u001b[0m, in \u001b[0;36mrequires_pyproj.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m HAS_PYPROJ:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyproj\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is required for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to work. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall it and initialize the object with a CRS before using it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImporting pyproj resulted in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpyproj_import_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/geopandas/array.py:1019\u001b[0m, in \u001b[0;36mGeometryArray.to_crs\u001b[0;34m(self, crs, epsg)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1017\u001b[0m transformer \u001b[38;5;241m=\u001b[39m TransformerFromCRS(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrs, crs, always_xy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1019\u001b[0m new_data \u001b[38;5;241m=\u001b[39m transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, transformer\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GeometryArray(new_data, crs\u001b[38;5;241m=\u001b[39mcrs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/geopandas/array.py:1749\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m   1746\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(data)\n\u001b[1;32m   1748\u001b[0m coords \u001b[38;5;241m=\u001b[39m shapely\u001b[38;5;241m.\u001b[39mget_coordinates(data[\u001b[38;5;241m~\u001b[39mhas_z], include_z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1749\u001b[0m new_coords_z \u001b[38;5;241m=\u001b[39m func(coords[:, \u001b[38;5;241m0\u001b[39m], coords[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1750\u001b[0m result[\u001b[38;5;241m~\u001b[39mhas_z] \u001b[38;5;241m=\u001b[39m shapely\u001b[38;5;241m.\u001b[39mset_coordinates(\n\u001b[1;32m   1751\u001b[0m     data[\u001b[38;5;241m~\u001b[39mhas_z]\u001b[38;5;241m.\u001b[39mcopy(), np\u001b[38;5;241m.\u001b[39marray(new_coords_z)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1752\u001b[0m )\n\u001b[1;32m   1754\u001b[0m coords_z \u001b[38;5;241m=\u001b[39m shapely\u001b[38;5;241m.\u001b[39mget_coordinates(data[has_z], include_z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/wf/lib/python3.12/site-packages/pyproj/transformer.py:840\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, xx, yy, zz, tt, radians, errcheck, direction, inplace)\u001b[0m\n\u001b[1;32m    838\u001b[0m     intime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# call pj_transform.  inx,iny,inz buffers modified in place.\u001b[39;00m\n\u001b[0;32m--> 840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[1;32m    841\u001b[0m     inx\u001b[38;5;241m=\u001b[39minx,\n\u001b[1;32m    842\u001b[0m     iny\u001b[38;5;241m=\u001b[39miny,\n\u001b[1;32m    843\u001b[0m     inz\u001b[38;5;241m=\u001b[39minz,\n\u001b[1;32m    844\u001b[0m     intime\u001b[38;5;241m=\u001b[39mintime,\n\u001b[1;32m    845\u001b[0m     direction\u001b[38;5;241m=\u001b[39mdirection,\n\u001b[1;32m    846\u001b[0m     radians\u001b[38;5;241m=\u001b[39mradians,\n\u001b[1;32m    847\u001b[0m     errcheck\u001b[38;5;241m=\u001b[39merrcheck,\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# if inputs were lists, tuples or floats, convert back.\u001b[39;00m\n\u001b[1;32m    850\u001b[0m outx \u001b[38;5;241m=\u001b[39m _convertback(x_data_type, inx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_affected_list = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected = ex.find_num_people_affected(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected['year'] = start_year + i\n",
    "    num_affected_list.append(num_affected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we added a year variable to each output, we can concatonate these dataframes together, and then look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now we'll join those dataframes together. \n",
    "num_affected_df = pd.concat(num_affected_list, axis=0)\n",
    "num_affected_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output has three columns: ID_hazard, num_people_affected, and year. \n",
    "We added year, but the other two are output from find_num_people_affected.\n",
    "\n",
    "To find the total number of people affected by any wildfire disaster by year, \n",
    "we could group this output dataframe by year and sum. But, we're about to do two more interesting examples, so we'll leave this output as is, and dive into the output from the later examples in the third section of this tutorial. \n",
    "\n",
    "In our output, one thing has changed: our ID_hazard column is not the same as \n",
    "the ID_hazard column that we started with. \n",
    "\n",
    "We wanted to count the number of people residing within 10km of *any* \n",
    "US wildfire disaster. There are some people who live within 10km of two or\n",
    "more wildfire disasters. Because we just wanted the total, we did not want to \n",
    "double count those people. When computing a total, rather than the number of \n",
    "people affected by each unique hazard, find_num_people_affected takes the unary \n",
    "union of any buffered hazards that are overlapping, and finds the total of \n",
    "everyone residing within that area. In the output, the hazard IDs of any \n",
    "overlapping hazards are concatenated. This avoids double counting, while still \n",
    "giving the user as much information about how many people lived near each \n",
    "hazard or group of hazards as possible. \n",
    "\n",
    "We can save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_df.to_parquet(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \n",
    "We also wanted to 2. Find the total number of people residing within 10 km of \n",
    "*EACH* US wildfire disaster in 2016, 2017, and 2018. \n",
    "\n",
    "To do this, we also need to use find_num_people_affected, with all the same \n",
    "arguments except for by_unique_hazard. In this case, we set by_unique_hazard to \n",
    "True. This means that we will count the number of people within 10km of each \n",
    "wildfire disaster boundary, regardless of whether two or more exposed areas \n",
    "overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list_unique_hazard = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected_unique = ex.find_num_people_affected(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=True # setting by unique hazard to True \n",
    "    )\n",
    "    num_affected_unique['year'] = start_year + i\n",
    "    num_affected_list_unique_hazard.append(num_affected_unique)\n",
    "\n",
    "num_affected_unique = pd.concat(num_affected_list_unique_hazard, axis=0)\n",
    "num_affected_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our output has three columns: ID_hazard, num_people_affected, and year. \n",
    "\n",
    "This time, the ID_hazard column is the same as the one we passed to this \n",
    "function. This time, if people lived within 10 km of one or more fires, they\n",
    "are counted in the total people affected by that fire. This means people may\n",
    "be double counted or triple or more. \n",
    "\n",
    "We can save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_unique.to_parquet(data_dir / \"03_results\" / \"num_aff_by_unique_wildfire.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two more ways we wanted to define exposure. \n",
    "\n",
    "3. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "\n",
    "These are analogous to the two quantities we just computed, but this time, we want to know these exposures by ZCTA. \n",
    "\n",
    "To do this, we need to run find_number_of_people_affected_by_geo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. \n",
    "To do \"3. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA\", we'll run run find_number_of_people_affected_by_geo with by_unique_hazard = False.\n",
    "\n",
    "This time, because we need to read in the large ZCTA file, the run will take slightly longer. It should take about 4 minutes, with the majority of this run time being the time it takes to read in the ZCTA file. If you're running these functions on giant datasets, we recommend parallelizeing over space or time. Using Parquet files instead of GeoJSON will also speed up this process, but if you're writing geodataframes out of R, as of now, the sfarrow package is not fully developed and integrated into R yet and writing Parquets can cause errors, so be careful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_zcta_list = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected_zcta = ex.find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to False \n",
    "    )\n",
    "    num_affected_zcta['year'] = start_year + i\n",
    "    num_affected_zcta_list.append(num_affected_zcta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case where we were not computing by ZCTA, the results come with \n",
    "hazard ids for groups of overlapping hazards.\n",
    "\n",
    "Since we're interested in the total number of people affected by ZCTA, we'll \n",
    "group the output dataframe by ZCTA and year and sum over any hazard IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all years into one dataframe\n",
    "num_affected_zcta_df = pd.concat(num_affected_zcta_list, axis=0)\n",
    "num_affected_zcta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_num_af = num_affected_zcta_df.groupby([\"ID_spatial_unit\", \"year\"]).agg(\n",
    "     {\"num_people_affected\": \"sum\"}\n",
    ").reset_index()\n",
    "\n",
    "# And we can save \n",
    "agg_num_af.to_parquet(data_dir / \"03_results\" / \"num_people_affected_by_wildfire_by_zcta.parquet\")\n",
    "agg_num_af.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \n",
    "For our final case of counting exposed people, in which we aim to find the number of people affected by each hazard by each ZCTA, we do the same as we just did, but with by_unique_hazard = True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_zcta_unique_list = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected_zcta_unique = ex.find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=True # setting by unique hazard to True \n",
    "    )\n",
    "    num_affected_zcta_unique['year'] = start_year + i\n",
    "    num_affected_zcta_unique_list.append(num_affected_zcta_unique)\n",
    "\n",
    "# all years in one dataframe\n",
    "num_affected_df_zcta_unique = pd.concat(num_affected_zcta_unique_list, axis=0)\n",
    "\n",
    "# and we can save\n",
    "num_affected_df_zcta_unique.to_parquet(data_dir / \"03_results\" / \"num_people_affected_by_wildfire_zcta_unique.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore some of the output from these runs in the next section of the tutorial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \n",
    "\n",
    "Finally, let's use the function find_number_of_people_residing_by_geo to get some denominators for our dataset. This function can help us use the gridded population data we used to find the number of people residing within the hazard buffers to also find the number of people residing in each ZCTA. This is useful if we're using a gridded population dataset that we think is a big improvement over other population counts in our additional spatial units, or we just want to be consistent. \n",
    "\n",
    "To call this function, all we need to do is use the same paths we've used previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_residing_by_zcta = ex.find_number_of_people_residing_by_geo(\n",
    "    path_to_additional_geos=zcta_path,\n",
    "    raster_path=ghsl_path\n",
    ")\n",
    "\n",
    "num_residing_by_zcta.to_parquet(data_dir / \"03_results\" / \"num_people_residing_by_zcta.parquet\")\n",
    "num_residing_by_zcta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we have a column for spatial unit and a column for the number of people living in that spatial unit. \n",
    "Please continue to part 3 of this tutorial to explore the output of these functions! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
