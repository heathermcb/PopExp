{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction \n",
    "\n",
    "The purpose of this notebook, along with 02_demo_example_run.ipynb and 03_demo_explore_results.ipynb, is to provide a tutorial of how you may want to use the PopExp package functions.\n",
    "\n",
    "This notebook cleans some raw data files for use in the PopExp functions, and then the subsequent two notebooks run the functions and explore results. \n",
    "\n",
    "Prerequisites: This tutorial assumes that you have a version of Python installed on your computer compatible with the requirements of PopExp, you have an IDE, and youâ€™re able to open and run a Jupyter notebook as well as Python scripts. Congrats! If you're reading this you probably opened the notebook and hopefully can run it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lauren - I'm missing instructions for how to install pop_exp.yml. I think you had some already for wf.yml so I'll leave this part to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are we doing in this tutorial? \n",
    "\n",
    "This tutorial will teach you how to use PopExp to find the number of people residing near wildfire disasters, and additionally the number of people by residing near wildfire disasters by ZCTA, across the years 2016-2018. It will disucss in more detail how PopExp allows you to define exposure in different ways shortly.  \n",
    "\n",
    "PopExp stands for Population Exposure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data used in this tutorial\n",
    "\n",
    "Functions available in PopExp allow the user to find the number of people who reside near environmental hazards like wildfires, either inside the hazard boundaries or within a buffer of the environmental hazard, using a dataset of hazards and a gridded population dataset. If the user wishes to find the number of people affected within additional spatial units such as ZCTAs or census tracts, a dataset describing those geographies is also required. \n",
    "\n",
    "In this example, we'll use a publicly available dataset of US wildfire disaster boundaries for the years 2016-2018 as our hazard data. A description of this data and a link for download is available here: \n",
    "\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/R73R85\n",
    "\n",
    "This tutorial will demonstrate how to find the number of people residing within a buffer of these wildfires by ZCTA, so we'll also use a shapefile of 2020 ZCTAs, described in detail and available for download here:\n",
    "\n",
    "https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n",
    "\n",
    "For the required gridded population data, which is used by the function to determine how many people live where, we'll use the version of the Global Human Settlement Layer which describes the residential population of the globe at 100 m resolution. It's downloadable here: \n",
    "\n",
    "https://human-settlement.emergency.copernicus.eu/download.php?ds=pop\n",
    "\n",
    "If you want to run this tutorial yourself, you can create a directory within the demo folder called demo_data, and put these files into a subfolder of the demo_data folder called 01_raw_data. You'll also need to create 02_interim_data, and 03_results, as subdirectories of the demo_data folder, since this tutorial will populate those folders. If you create these directories, the path names written here should reference these files correctly. \n",
    "\n",
    "## EDIT: we could write some code to create these for people\n",
    "## EDIT: \"It also assumes that you are able to create and manage a virtual environment into which you have installed the Python package pop_exp.\" might want to add how to activate the environment we've created for them. idk i'm kind of thinking we should leave this step up to the users. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can PopExp do? \n",
    "\n",
    "The Python package PopExp can do five distinct computations. \n",
    "\n",
    "1. Find the total number of people who reside within a buffer distance (which \n",
    "can be 0) of *any* environmental hazard in a set of hazards.\n",
    "2. Find the total number of people who reside within a buffer distance (which \n",
    "can be 0) of *EACH* environmental hazard in a set of hazards.\n",
    "3. Find the total number of people who reside within a buffer distance (which \n",
    "can be 0) of *any* environmental hazard in a set of hazards, by additional spatial \n",
    "unit (ex. the total number of people who resided within 10km of any wildfire \n",
    "disaster in 2018 by ZCTA).\n",
    "4. Find the total number of people who reside within a buffer distance (which \n",
    "can be 0) of *EACH* environmental hazard in a set of hazards, by additional \n",
    "spatial unit. \n",
    "5. Find the number of people residing within each spatial unit according to a\n",
    "gridded population dataset. \n",
    "\n",
    "The fifth function is meant to provide denominators for computations conducted \n",
    "with the first 4. For example, you may want to find the total number of people \n",
    "who resided within 10km of any wildfire disaster in 2018 by ZCTA, and then \n",
    "calculate the proportion of the ZCTA population that was exposed. To do this, \n",
    "you could use a function in PopExp to find the ZCTA population according to \n",
    "the gridded population raster you used to determine exposure. \n",
    "\n",
    "This tutorial will demonstrate all of these options. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demo all the functions available in PopExp, we will do five separate computations, which align with the five options available in the package. \n",
    "\n",
    "1. Find the total number of people residing within 10km of *ANY* US wildfire \n",
    "disaster in 2016, 2017, and 2018. \n",
    "2. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018.\n",
    "3. Find the total number of people residing within 10km of *ANY* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "5. Find the population of all 2020 ZCTAs. \n",
    "\n",
    "To do the first two computations, we need to call the pop_exp function \n",
    "find_number_of_people_affected. The function parameter by_unique_hazard allows\n",
    "us to specify whether we want to find the total number of people residing near \n",
    "*any* hazard, or *each* hazard. When by_unique_hazard = False, the function \n",
    "computes the number of people exposed to any hazard. When it's True, it computes\n",
    "a total for each unique hazard. \n",
    "\n",
    "To do the second computations, we'll call the function \n",
    "find_number_of_people_affected_by_geo. As with find_number_of_people_affected, \n",
    "when by_unique_hazard = False, the function computes the number of people \n",
    "exposed to any hazard by ZCTA, and when it's True, it computes\n",
    "a total number of people affected for each unique hazard by ZCTA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "To prepare to do the above computations, we need to prepare the data. \n",
    "This is where we start coding! First we import some libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by preparing the wildfire data. We'll set directories, and then \n",
    "read in the raw wildfire data as downloaded from Harvard dataverse. Note that\n",
    "the raw data contains wildfire disasters for years 2000-2019, which is a lot, \n",
    "and we're going to filter down to only 2016-2018 for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = pathlib.Path.cwd().parent\n",
    "data_dir = base_path / \"demo_data\"\n",
    "\n",
    "fires = gpd.read_file(data_dir / \"01_raw_data\"/ \"wildfires_conus.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot the data to make sure it read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both find_num_people_affected, and find_number_of_people_affected_by_geo, \n",
    "when we run the functions, we need to pass a path to a hazard dataframe with \n",
    "3 columns:  ID_climate_hazard, buffer_dist, and geometry. So we need to decide \n",
    "on a buffer distance and create that column, and rename the other columns to \n",
    "the correct names. \n",
    "\n",
    "For this tutorial, we've decided we want to consider people exposed to a wildfire\n",
    "if they live within 10 km of the boundaries of the wildfire disasters that are\n",
    "specified in my dataset. You could do something different if you think the \n",
    "relevant distance from your hazard is different. You can also assign a buffer\n",
    "of 0 to your hazards, or different buffers to each hazard in your dataset. They\n",
    "don't all have to be the same. You could even assign buffers based on the hazard area, or another characteristic of each hazard.\n",
    "\n",
    "The buffer distance is in meters, so we'll specify a 10,000 m buffer distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires[\"buffer_dist\"] = 10000 # buffer distance in in meters \n",
    "fires.head # Checking what columns I have in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a buffer distance column. We need to select and rename the\n",
    "remaining columns we need, but we also need to select the years \n",
    "we're interested in. \n",
    "\n",
    "Here, we're interested in years 2016-2018, and we want to determine \n",
    "exposure by year. We want to compute the total number of people affected by any \n",
    "fire in 2016, 2017, and 2018, as well as aply the three other exposure definitions we\n",
    "wrote out above yearly.\n",
    "\n",
    "There is no option in pop_exp to indicate which hazards are\n",
    "for which year, or time period. If I want to know the total number of people \n",
    "affected by hazards in 2016 but not 2017, I need to feed pop_exp the exposure\n",
    "data for 2016 along with a gridded population dataset that represents the \n",
    "population in 2016. If I wanted monthly exposure for 2016, I'd need to split\n",
    "my exposure data up by month and call the function separately on each month. \n",
    "\n",
    "In this tutorial, we'll use the GHSL data from 2020 for each year 2016-2018, since it's close enough, but split up the hazard data because we want yearly counts.\n",
    "\n",
    "So before selecting just the ID, hazard, and buffer distance columns, we're going to select and split up the years we're interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fires in 2016, 2017, 2018\n",
    "fires = fires[fires[\"wildfire_year\"].isin([2016, 2017, 2018])]\n",
    "# Split this into a list of dataframes by year\n",
    "fires_by_year = [fires[fires[\"wildfire_year\"] == year] for year in [2016, 2017, 2018]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our exposure datasets, we'll select and rename the columns we need\n",
    "for pop_exp: ID_climate_hazard, buffer_dist, and geometry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, select cols\n",
    "fires_by_year = [fire[[\"wildfire_id\", \"buffer_dist\", \"geometry\"]] for fire in fires_by_year]\n",
    "# Then rename the wildfire ID col\n",
    "fires_by_year = [fire.rename(columns={\"wildfire_id\": \"ID_climate_hazard\"}) for fire in fires_by_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write these out into an interim data folder to call \n",
    "using the pop_exp function, since the pop_exp function requires you to pass \n",
    "a path name, not an object in Python. We're using geojson files because these functions require either GEOJSON or Parquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fire in enumerate(fires_by_year):\n",
    "    fire.to_file(data_dir / \"02_interim_data\" / f\"wildfires_{2016 + i}.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've dealt with the wildfire disaster exposure data which is going to be our\n",
    "\"hazard\" data for pop exp. Now we also need to get the ZCTA data into the right\n",
    "format. \n",
    "\n",
    "We've chosen to use 2020 ZCTA data, since the time period 2016-2018 is closer to the 2020 census than the 2010 census. We'll read in the data, and then select and rename the columns to be what the functions find_number_of_people_affected and find_number_of_people_affected_by_geo require. \n",
    "\n",
    "We need to rename the ZCTA ID to 'ID_spatial_unit'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I think this ZCTA file is somehow preprocessed - might want to change back to raw and deal with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm reading in the raw ZCTA data. \n",
    "zctas = gpd.read_file(data_dir / \"01_raw_data\" / \"tl_2020_us_zcta520\" / \"tl_2020_us_zcta520.shp\")\n",
    "\n",
    "# Rename \n",
    "zctas.rename(columns={\"ZCTA5CE20\": \"ID_spatial_unit\"}, inplace=True)\n",
    "# select ID_spatial_unit and geometry\n",
    "zctas = zctas[[\"ID_spatial_unit\", \"geometry\"]].copy()\n",
    "zctas.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll save this as a geojson file as well. This additional spatial unit \n",
    "file can also be in GEOJSON or Parquet format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a minute. \n",
    "zctas_path = data_dir / \"02_interim_data\" / \"zctas_2020.geojson\"\n",
    "zctas.to_file(zctas_path, driver = 'GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the gridded population raster doesn't require any preprocessing, our \n",
    "data is ready! Proceed to 02_demo_example_run.ipynb to continue the tutorial. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
