{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION:\n",
    "\n",
    "The purpose of this notebook, along with 01_data_setup_example.ipynb, is to \n",
    "provide a tutorial of how you may want to use the pop_exp pacakge functions.\n",
    "\n",
    "Please see 01_data_setup_example.ipynb before you work through this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOALS:\n",
    "\n",
    "To recap, to demo all the functions available in pop_exp, in this notebook we'll\n",
    "use the pop_exp functions to do five separate things, which align with the \n",
    "five options available in the package. \n",
    "\n",
    "1. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018. \n",
    "2. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018.\n",
    "3. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "5. Find the population of all 2020 ZCTAs. \n",
    "\n",
    "In the last notebook, we prepared the wildfire disaster exposure data and ZCTA\n",
    "data to pass to the pop_exp functions so we could complete these computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S DO IT:\n",
    "\n",
    "We need to import some libraries and also install and import pop_exp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing necessary libraries.\n",
    "import pathlib\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "#import pop_exp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER IMPORT STATEMENT \n",
    "code_path = '/Users/heathermcbrien/Documents/Documents/GitHub.nosync/casey_lab_shared_functions/pop_ex/code'\n",
    "# these statements together import all the functions defined in the Pop Ex \n",
    "# module.\n",
    "sys.path.append(str(code_path))\n",
    "from pop_ex_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set some paths to make it easy to access the data we cleaned for \n",
    "this tutorial. \n",
    "\n",
    "To find the number of people affected by one or more wildfire disaster \n",
    "by ZCTA by year 2016-2018, we need to get the paths to each of our wildfire \n",
    "files that we made in the data setup notebook.\n",
    "\n",
    "The regular expression below lists all the files in the interim data direcotry, \n",
    "and we're selecting the first three files, which are the wildfire disaster \n",
    "data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "base_path = pathlib.Path.cwd().parent\n",
    "data_dir = base_path / \"demo_data\"\n",
    "# wf paths regex\n",
    "wildfire_paths = sorted(glob.glob(str(data_dir / \"02_interim_data\" /  \"*\")))[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the path to the population raster we're using, and the ZCTA file.\n",
    "For me, my local path to where I have downloaded the GHSL dataset is below. \n",
    "Replace with your local path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghsl_path = 'MY_LOCAL_PATH/GHSL/100 m/GHS_POP_E2020_GLOBE_R2023A_54009_100_V1_0/GHS_POP_E2020_GLOBE_R2023A_54009_100_V1_0.tif'\n",
    "\n",
    "# ZCTA path \n",
    "zcta_path = sorted(glob.glob(str(data_dir / \"02_interim_data\" /  \"*\")))[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now set up to run the four functions we're interested in. \n",
    "\n",
    "Our first goal was:\n",
    "1. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018. \n",
    "\n",
    "To do this, we can run find_number_of_people_affected with the parameter \n",
    "by_unique_hazrad = False. \n",
    "\n",
    "Because we're looping over three years, we'll initialize an empty list first, \n",
    "and then store the results in this list. We're also adding a year variable to \n",
    "the result as we go.\n",
    "\n",
    "It takes about 1 minute to run this function per year of wildfire data, for a \n",
    "total of 3 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list = []\n",
    "\n",
    "for i in range(0, 2):\n",
    "    num_affected = find_num_people_affected(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected['year'] = 2016 + i\n",
    "    num_affected_list.append(num_affected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we saved each output with a year variable, we can concatonate these \n",
    "dataframes together, and then look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now we'll join those dataframes together. \n",
    "num_affected_df = pd.concat(num_affected_list, axis=0)\n",
    "num_affected_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output has three columns: ID_hazard, num_people_affected, and year. \n",
    "We added year, but the other two are output from find_num_people_affected.\n",
    "\n",
    "To find the total number of people affected by any wildfire disaster by year, \n",
    "we could group this output dataframe by year and sum. \n",
    "\n",
    "In our output, one thing has changed: our ID_hazard column is not the same as \n",
    "the ID_hazard column that we started with. \n",
    "\n",
    "We wanted to count the number of people residing within 10km of *any* \n",
    "US wildfire disaster. There are some people who live within 10km of two or\n",
    "more wildfire disasters. Because we just wanted the total, we did not want to \n",
    "double count those people. When computing a total, rather than the number of \n",
    "people affected by each unique hazard, find_num_people_affected takes the unary \n",
    "union of any buffered hazards that are overlapping, and finds the total of \n",
    "everyone residing within that area. In the output, the hazard IDs of any \n",
    "overlapping hazards are concatenated. This avoids double counting, while still \n",
    "giving the user as much information about how many people lived near each \n",
    "hazard or group of hazards.\n",
    "\n",
    "We can save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected.to_csv(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wanted to find the total number of people residing within 10 km of \n",
    "*EACH* US wildfire disaster in 2016, 2017, and 2018. \n",
    "\n",
    "To do this, we also need to use find_num_people_affected, with all the same \n",
    "arguments except for by_unique_hazard. In this case, we set by_unique_hazard to \n",
    "True. This means that we will count the number of people within 10km of each \n",
    "wildfire disaster boundary, regardless of whether two or more exposed areas \n",
    "overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list_unique_h = []\n",
    "\n",
    "for i in range(0, 2):\n",
    "    num_affected_unique = find_num_people_affected(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected['year'] = 2016 + i\n",
    "    num_affected_list_unique_h.append(num_affected_unique)\n",
    "\n",
    "num_affected_unique = pd.concat(num_affected_list_unique_h, axis=0)\n",
    "num_affected_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our output has three columns: ID_hazard, num_people_affected, and year. \n",
    "\n",
    "This time, the ID_hazard column is the same as the one we passed to this \n",
    "function. This time, if people lived within 10 km of one or more fires, they\n",
    "are counted in the total people affected by that fire. This means people may\n",
    "be double counted or triple or more. \n",
    "\n",
    "We can save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected.to_csv(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two more ways we wanted to define exposure. These are analogous to \n",
    "the two quantities we just computed, but this time, we want to know these\n",
    "exposures by ZCTA. \n",
    "\n",
    "3. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "\n",
    "To do this, we need to run find_number_of_people_affected_by_geo. \n",
    "\n",
    "First, we'll find the total number of people affected by ZCTA \n",
    "(by_unique_hazard = False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list = []\n",
    "\n",
    "for i in range(0, 2):\n",
    "    num_affected = find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected['year'] = 2016 + i\n",
    "    num_affected_list.append(num_affected)\n",
    "\n",
    "# putting all years into one dataframe\n",
    "num_affected_df = pd.concat(num_affected_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case where we were not computing by ZCTA, the results come with \n",
    "hazard ids for groups of overlapping hazards.\n",
    "\n",
    "Since we're interested in the total number of people affected by ZCTA, we'll \n",
    "group the output dataframe by ZCTA and year and sum over any hazard IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_num_af = num_affected_df.groupby([\"ID_spatial_unit\", \"year\"]).agg(\n",
    "     {\"num_people_affected\": \"sum\"}\n",
    ").reset_index()\n",
    "\n",
    "# And we can save \n",
    "agg_num_af.to_csv(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of people affected by each hazard by each ZCTA, we do the \n",
    "same, but with by_unique_hazard = True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list = []\n",
    "\n",
    "for i in range(0, 2):\n",
    "    num_affected = find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=True # setting by unique hazard to true \n",
    "    )\n",
    "    num_affected['year'] = 2016 + i\n",
    "    num_affected_list.append(num_affected)\n",
    "\n",
    "# all years in one dataframe\n",
    "num_affected_df = pd.concat(num_affected_list, axis=0)\n",
    "# and we can save\n",
    "agg_num_af.to_csv(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now we're going to run the function. We're going to loop through the \n",
    "# wildfire paths in order to run it on each year 2016-2018. \n",
    "\n",
    "# We're going to run find_num_people_affected_by_geo, because we want to find \n",
    "# the number of people by ZCTA, and we're going to set the by_unique_hazard \n",
    "# argument to False, since we want to get the total number of people affected by \n",
    "# any wildflire hazard in each ZCTA, not the number of people affected by each \n",
    "# fire in each ZCTA. \n",
    "\n",
    "# We'll initalize an empty list first, and store the results of each run of the \n",
    "# loop in this list. We're also adding a year variable to the result as we go.\n",
    "\n",
    "# It takes about 1 minute to run per year of wildfire data, for a \n",
    "# total of 3 minutes.\n",
    "\n",
    "# If you want to run on a lot of data, we recommend parallelizing over time or \n",
    "# space.  \n",
    "\n",
    "num_affected_list = []\n",
    "\n",
    "for i in range(0, 2):\n",
    "    num_affected = find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected['year'] = 2016 + i\n",
    "    num_affected_list.append(num_affected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now we'll join those dataframes together. \n",
    "num_affected_df = pd.concat(num_affected_list, axis=0)\n",
    "\n",
    "# The results also come with hazard ids for groups of overlapping hazards. \n",
    "# Since we just want results by ZCTA, we'll group by ZCTA and year and sum \n",
    "# over any hazard IDs.\n",
    "agg_num_af = num_affected_df.groupby([\"ID_spatial_unit\", \"year\"]).agg(\n",
    "     {\"num_people_affected\": \"sum\"}\n",
    ").reset_index()\n",
    "\n",
    "# And we can save \n",
    "agg_num_af.to_csv(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the data if we want - but that's that!\n",
    "print(agg_num_af)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
